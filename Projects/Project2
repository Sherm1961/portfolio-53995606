---
title: "COD_Final_Proj"
author: "Evan Arias-Johnson/Sherman LaCost"
date: "2024-11-18"
output: word_document
---


```{r}

#library
library(dplyr)
library(ggplot2)
library(caret)
library(class)
library(rpart) 
library(rpart.plot)
library(dplyr)
library(ggpubr) #PLEASE ADD THEM 
library(factoextra) #PLEASE ADD THEM 
library(corrplot)

```
Intialize the data:
```{r}
#setwd("C:\\Users\\alpha\\OneDrive\\Desktop\\Pred_Analytics\\Final Project")
data <- read.csv("cod.csv")
summary(data) #summary to overlook data
```



```{r}
#Main focus now is clean data based on KD, score per min, headshots and time played. Get rid of outliers and
#make cleaned dataset "data_new".
#Removed players with less than 10 games played as you cant get an accurate read on skill (may have previous
#expirience or they have fluke games skewing their game metrics)

#We want to make a weighted value of player skill, would be categorical (Beginner, Intermediate, Advanced, Skilled)
#to see how skilled players are


#KD, score per min, headshots and time played.

#kdRatio dataset
# Filter rows where kdRatio is exactly 0 and deaths is 0
zero_kd_data <- data[!is.na(data$kdRatio) &
                       data$kdRatio == 0 & data$deaths == 0, ]

# Display the players with 0 kdRatio
zero_kd_players <- zero_kd_data$`name`
#print(zero_kd_players)

# Remove rows from data that have zero_kd_data
data_filtered_zkd <- anti_join(data, zero_kd_data, by = c("kdRatio", "name"))
#print(data_filtered_zkd)




#headshots dataset
# Filter rows where headshots is exactly 0 and kills is 0
zero_headshots_data <- data[!is.na(data$headshots) &
                              data$headshots == 0 & data$kills == 0, ]

# Display the players with 0 headshots
zero_headshots_players <- zero_headshots_data$`name`
#print(zero_headshots_players)

# Remove rows from data that have zero_headshots_data
data_filtered_zhs <- anti_join(data, zero_headshots_data, by = c("headshots", "name"))
#print(data_filtered_zhs)



#score per minute dataset
# Filter rows where score per minute is exactly 0 and kills is 0
zero_score_data <- data[!is.na(data$scorePerMinute) &
                          data$scorePerMinute == 0 & data$kills == 0, ]

# Display the players with 0 scoreperminute
zero_score_players <- zero_score_data$`name`
#print(zero_score_players)

# Remove rows from data that have zero_score_data
data_filtered_zspm <- anti_join(data, zero_score_data, by = c("scorePerMinute", "name"))
#print(data_filtered_zspm)



#time played dataset
# Filter rows where timePlayed is exactly 0
zero_time_data <- data[!is.na(data$timePlayed) &
                         data$timePlayed == 0, ]

#a_zero_time<- count(data[!is.na(data$timePlayed) & data$timePlayed == 0, ]) #210 with 0 hours played
#a_0_5_time<- count(data[!is.na(data$timePlayed) & data$timePlayed <= 5, ]) #423 with 5 or less hours
# Display the players with 0 timePlayed
zero_time_players <- zero_time_data$`name`
#print(zero_time_players)

# Remove rows from data that have zero_time_data
data_filtered_ztp <- anti_join(data, zero_time_data, by = c("timePlayed", "name"))
#print(data_filtered_ztp)



#gamesPlayed dataset
# Filter rows where gamesPlayed is 10 or less
zero_gp10_data <- data[!is.na(data$gamesPlayed) &
                         data$gamesPlayed <= 10, ]

# Display the players with 0 games Played
zero_gp10_players <- zero_gp10_data$`name`
#print(zero_gp10_players)

# Remove rows from data that have zero_gp_data
data_filtered_gp10 <- anti_join(data, zero_gp10_data, by = c("gamesPlayed", "name"))
#print(data_filtered_gp10)



#remove data using anti_join to make new data
data_new <- anti_join(data, zero_kd_data, by = c("kdRatio", "name"))
data_new <- anti_join(data_new, zero_time_data, by = c("timePlayed", "name"))
data_new <- anti_join(data_new, zero_score_data, by = c("scorePerMinute", "name"))
data_new <- anti_join(data_new, zero_headshots_data, by = c("headshots", "name"))
data_new <- anti_join(data_new, zero_gp10_data, by = c("gamesPlayed", "name"))

summary(data_new)

```

The next part is to create new variables in the dataset to help us do our analysis. In this case that is:
- winRate (Wins/(Wins + Losses))
- hitAccuracy (Hits/(Shots))
- critAccuracy (Headshots/(Shots))
- Kills_Per_Game [KPG] (Kills/Games_Played)
- Assists_Per_Game [APG] (Assits/Games_Played)

The goal here is to gauge a player's mechanical ability and their overall active participation with their team. 

```{r}
data_new$winRate <- round((data_new$wins/data_new$gamesPlayed)*100,2) #Win Rate
data_new$hitAccuracy <- round((data_new$hits/data_new$shots)*100,2) #Percent Chance of player hitting target
data_new$critAccuracy <- round((data_new$headshots/data_new$shots)*100,2) #Percent chance of player hitting crit/headshot
data_new$KPG <- round((data_new$kills/data_new$gamesPlayed),2) #Average kills per game
data_new$APG <- round((data_new$assists/data_new$gamesPlayed),2) #Average Assists per game

```


We are going to have to do some more data cleaning based on the results of two of the columns: 
- KPG 
- Win Rate 

Mainly because there is someone who has more wins than games played and another has 300+ kills a game and only has 13 games played. 

```{r}
#Filter who has more wins than games played
MoreWins <- data[data$wins >= data$gamesPlayed,]

#Anti-Join for those values
data_new <- anti_join(data_new, MoreWins, by = c("gamesPlayed", "name"))

#Filter for outliers of kills per game
KPG_Outlier <- data_new[data_new$KPG >= 51,]

#Anti-Join for those values
data_new <- anti_join(data_new, KPG_Outlier, by = c("KPG", "name"))
```


```{r}
#Cleaned data v1
#data_new
```


I am going to create a subset of the dataframe that has the values I care about and then scale them. 
The columns I are about are: Wins, Kills, KDRatio, Killstreak, Losses, hits, headshots, gamesplayed, assists, misses, shots, deaths, winRate, hitAcc, critAcc, KPG and APG

```{r}
 Data_Scale <- data_new[, c(2, 3, 4, 5, 7, 9, 11, 13, 14, 15, 17, 18, 19, 20, 21,22,23, 24)]
#print(Data_Scale)

Data_Scale <- data.frame(scale(Data_Scale))
#print(Data_Scale)


Data_Scale$TotalScale <- round(rowSums(Data_Scale[,1:18]),2)
Data_Scale$AvgScale <- round(rowMeans(Data_Scale[,1:18]),2)
#print(Data_Scale)


# Create a sample correlation matrix
correlation_matrix <- cor(Data_Scale)

# Visualize the correlation matrix
corrplot(correlation_matrix)
```


This does a linear regression test for Scaled values to win rate
```{r}
# Fit the Linear Regression Model using columns from both data frames 
#model1 <- lm(scale(Data_Scale$TotalScale) ~ scale(data_new$winRate)) 
model1 <- lm(scale(data_new$winRate) ~ scale(Data_Scale$TotalScale)) 
#Summarize the Model 
summary(model1)


#This Generates the plot of the Linear regression model 
plot( scale(data_new$winRate), scale(Data_Scale$TotalScale), main = "Linear Regression Model", xlab = "Win Rate", ylab = " Scaled Values", pch = 19, col = "blue")

#Creates the regression line for the model 
abline(model1,col = "red", lwd =2)
#Plots the model 
plot(scale(data_new$winRate) ~ scale(Data_Scale$TotalScale))
#Generates a histogram of the model 
hist(scale(Data_Scale$TotalScale), breaks = 30)
#Summary of skill
```


```{r}
print(summary(Data_Scale$TotalScale))




# Fit the Linear Regression Model using columns from both data frames 
model2 <- lm(Data_Scale$AvgScale ~ data_new$winRate) 
#Summarize the Model 
summary(model2)


#This Generates the plot of the Linear regression model 
plot( data_new$winRate, Data_Scale$AvgScale, main = "Linear Regression Model", xlab = "Win Rate", ylab = " Scaled Values", pch = 19, col = "blue")

#Creates the regression line for the model 
abline(model2,col = "red", lwd =2)
#Plots the model 
plot(Data_Scale$AvgScale ~ data_new$winRate)
#Generates a histogram of the model 
hist(Data_Scale$AvgScale, breaks = 30)
#Summary of skill 
print(summary(Data_Scale$AvgScale))
```

This does a kluster model for skilled and unskilled players. 
```{r}

set.seed(20)
#Does a K-Means model => Breaks the data into Skilled and Unskilled (??)
Data_Scale_Avg <- data.frame(Data_Scale[,-19])
Skill_Brack_Avg <- kmeans(Data_Scale_Avg[,c(14,19)], 3, iter.max = 10, nstart = 1)
print(Skill_Brack_Avg)

#Plots the K-Means Model for AVG
Avg_Cluster_Model <- fviz_cluster(Skill_Brack_Avg, data = Data_Scale_Avg[,c(14,19)],
             palette = c("#00AFBB", "#E7B800","#8c00f0"),
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )

 Avg_Cluster_Model <- Avg_Cluster_Model + 
   labs(title = " Avg Cluster Plot", 
        x = "Win Rate", 
        y = "Avg Scaled Values")

 
print(Avg_Cluster_Model)


# Section focused on the Sum version 
set.seed(20)
Data_Scale_Sum <- data.frame(Data_Scale[,-20])
#Does a K-Means model => Breaks the data into Skilled and Unskilled (??)
 Skill_Brack_Sum<- kmeans(Data_Scale_Sum[,c(14,19)], 3, iter.max = 10, nstart = 1)
 print(Skill_Brack_Sum)

#Plots the K-Means Model for SUM
 Sum_Cluster_Model <- fviz_cluster(Skill_Brack_Sum, data = Data_Scale_Sum[,c(14,19)],
             palette = c("#00AFBB", "#E7B800","#8c00f0"),
             geom = "point",
             ellipse.type = "convex", 
           ggtheme = theme_bw()
            )
 
 Sum_Cluster_Model <- Sum_Cluster_Model + 
   labs(title = " Sum Cluster Plot", 
        x = "Win Rate", 
        y = "Sum Scaled Values")
 
 
 Sum_Cluster_Model <- Sum_Cluster_Model + 
   scale_color_manual(values = c("#00AFBB", "#E7B800", "#8c00f0"), 
                      labels = c("Cluster A", "Cluster B", "Cluster C"))
 
 
print(Sum_Cluster_Model)


#Adds the Cluster value to the AVG Dataframe 
Data_Scale_Avg$clusterValue_Avg <- as.factor(Skill_Brack_Avg$cluster)
#Adds the CLuster value to to the SUM Dataframe
Data_Scale_Sum$clusterValue_Sum <- as.factor(Skill_Brack_Sum$cluster)
 
#Adds both to the main Scaled Frame
Data_Scale$clusterValue_Sum <- as.factor(Skill_Brack_Sum$cluster)
Data_Scale$clusterValue_Avg <- as.factor(Skill_Brack_Avg$cluster)

#Prints all 3 frames for review
#print(Data_Scale_Avg)
#print(Data_Scale_Sum)
#print(Data_Scale)


```
```{r}
set.seed(20)
#Does a K-Means model => Breaks the data into Skilled and Unskilled (??)
Data_Scale_Avg <- data.frame(Data_Scale[,-19])
Skill_Brack_Avg_6 <- kmeans(Data_Scale_Avg[,c(14,19)], 6, iter.max = 10, nstart = 1)
print(Skill_Brack_Avg_6)

#Plots the K-Means Model for AVG
Avg_Cluster_Model_6 <- fviz_cluster(Skill_Brack_Avg_6, data = Data_Scale[,c(14,19)],
             palette = c("#00AFBB", "#E7B800","#8c00f0","green","red","violet"),
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )

 Avg_Cluster_Model_6 <- Avg_Cluster_Model_6 + 
   labs(title = " Avg Cluster Plot", 
        x = "Win Rate", 
        y = "Avg Scaled Values")
 
print(Avg_Cluster_Model_6)


Data_Scale_Avg$clusterValue_Avg_6 <- as.factor(Skill_Brack_Avg_6$cluster)
#print(Data_Scale)
```

```{r}
#Shows dataframe of all of values in Data_Scale_Avg
#Data_Scale_Avg

```
For group of 3
```{r}
knn_df_3 <- Data_Scale_Avg[,-c(20,22)]
#print(knn_df_3)

# Setting the seed for reproducibility
set.seed(11)
split_ratio <- 0.7 # Assuming a 70-30 split for training and validation

# Calculate the number of observations for training
train_indices_3 <- sample(1:nrow(knn_df_3), size = floor(split_ratio * nrow(knn_df_3)))

# Split the data
train_data_3 <- knn_df_3[train_indices_3, ]
test_data_3 <- knn_df_3[-train_indices_3, ]


# Splitting the dataset
knn_train_indices_3 <- sample(1:nrow(knn_df_3), size = floor(split_ratio * nrow(knn_df_3)))
train_data_knn_3 <- knn_df_3[knn_train_indices_3, ]
test_data_knn_3 <- knn_df_3[-knn_train_indices_3, ]
train_labels_3 <- knn_df_3$clusterValue_Avg[knn_train_indices_3]
test_labels_3 <- knn_df_3$clusterValue_Avg[-knn_train_indices_3]


# Preprocess the data (excluding the label column)
norm.values_3 <- preProcess(train_data_knn_3[, -20], method = c("center", "scale"))
train_norm_3 <- predict(norm.values_3, train_data_knn_3[, -20])
test_norm_3 <- predict(norm.values_3, test_data_knn_3[, -20])

# Ensure labels are factors with the same levels
train_labels_3 <- factor(train_labels_3)
test_labels_3 <- factor(test_labels_3, levels = levels(train_labels_3))


#Initialize a dataframe to store accuracy results 
  accuracy.df <- data.frame(k = 1:30, Accuracy = rep(0, 30)) 
  # Compute k-NN for different k values on the validation set 
  set.seed(11)
  for (i in 1:30) { knn.pred_3 <- knn(train = train_norm_3, test = test_norm_3, cl = train_labels_3, k = i) 
  accuracy.df[i, "Accuracy"] <- confusionMatrix(knn.pred_3, test_labels_3)$overall["Accuracy"] } 
  # Print the accuracy dataframe
  print(accuracy.df)

```
For Group of 6
```{r}
knn_df <- Data_Scale_Avg[,-c(20,21)]
#print(knn_df)

# Setting the seed for reproducibility
set.seed(11)
split_ratio <- 0.6 # Assuming a 70-30 split for training and validation

# Calculate the number of observations for training
train_indices <- sample(1:nrow(knn_df), size = floor(split_ratio * nrow(knn_df)))

# Split the data
train_data <- knn_df[train_indices, ]
test_data <- knn_df[-train_indices, ]



# Splitting the dataset
knn_train_indices <- sample(1:nrow(knn_df), size = floor(split_ratio * nrow(knn_df)))
train_data_knn <- knn_df[knn_train_indices, ]
test_data_knn <- knn_df[-knn_train_indices, ]
train_labels <- knn_df$clusterValue_Avg_6[knn_train_indices]
test_labels <- knn_df$clusterValue_Avg_6[-knn_train_indices]


# Preprocess the data (excluding the label column)
norm.values <- preProcess(train_data_knn[, -20], method = c("center", "scale"))
train_norm <- predict(norm.values, train_data_knn[, -20])
test_norm <- predict(norm.values, test_data_knn[, -20])

# Ensure labels are factors with the same levels
train_labels <- factor(train_labels)
test_labels <- factor(test_labels, levels = levels(train_labels))


#Initialize a dataframe to store accuracy results 
  accuracy.df <- data.frame(k = 1:30, Accuracy = rep(0, 30)) 
  # Compute k-NN for different k values on the validation set 
  set.seed(11)
  for (i in 1:30) { knn.pred <- knn(train = train_norm, test = test_norm, cl = train_labels, k = i) 
  accuracy.df[i, "Accuracy"] <- confusionMatrix(knn.pred, test_labels)$overall["Accuracy"] } 
  # Print the accuracy dataframe
  print(accuracy.df)

```

```{r}
knn.pred_3 <- knn(train = train_norm_3, test = test_norm_3, cl = train_labels_3, k = 4)

# Align levels of knn.pred to match test_labels
knn.pred <- factor(knn.pred_3, levels = levels(test_labels_3))

# Evaluate the model with confusion matrix
confusion <- confusionMatrix(knn.pred_3, test_labels_3)
print(confusion)

```


